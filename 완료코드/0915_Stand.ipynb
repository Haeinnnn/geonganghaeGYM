{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Image Import\n",
        "\n",
        "- stand1500 : https://drive.google.com/drive/folders/1YGz7J9x5LE3QKOB6-HJi_--NIH2UxZio?usp=sharing\n",
        "\n",
        "구글 드라이브에 업로드"
      ],
      "metadata": {
        "id": "q0R7FUfnLDbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYYlz51GK8jc",
        "outputId": "e36b4c02-595f-4c9f-d0d1-542fc83e0335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') # 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/DATA/STAND1500/TRAIN\n",
        "import os\n",
        "dr= os.getcwd() #현재 위치 주소\n",
        "drD=\"\"\n",
        "drD=dr+\"/*/*.jpg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGBMc8MAN8OR",
        "outputId": "edfe95ce-2864-4adf-9bbb-cc4c725d7198"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1zaWNDIbEB7mz9mfiWjgvZ9_Vanr9-sSZ/DATA/STAND1500/TRAIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd # 현위치 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UBKjuwXhOsDr",
        "outputId": "29252bba-af6d-4d7c-e925-7ea5b70cd8d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/.shortcut-targets-by-id/1zaWNDIbEB7mz9mfiWjgvZ9_Vanr9-sSZ/DATA/STAND1500/TRAIN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading library\n",
        "\n",
        "- YOLO 모델 : 영역을 표시하고 뿐만 아니라 인식된 사물이 어떤것인지 까지 알려주는 모델"
      ],
      "metadata": {
        "id": "wpmywO-oPmXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/dhrim/darkeras-yolov4\n",
        "%cd darkeras-yolov4\n",
        "!wget -O weights/yolov3.weights https://pjreddie.com/media/files/yolov3.weights"
      ],
      "metadata": {
        "id": "Tko9t4L5PlH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9a1568-8160-4a59-fc82-eba4be85e3a3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'darkeras-yolov4' already exists and is not an empty directory.\n",
            "/content/darkeras-yolov4\n",
            "--2022-09-15 07:05:44--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘weights/yolov3.weights’\n",
            "\n",
            "weights/yolov3.weig 100%[===================>] 236.52M  62.4MB/s    in 4.1s    \n",
            "\n",
            "2022-09-15 07:05:48 (58.2 MB/s) - ‘weights/yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model loading"
      ],
      "metadata": {
        "id": "OY8wYSGNQ_c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yolov3_wrapper\n",
        "model = yolov3_wrapper.YoloV3Wrapper(\"weights/yolov3.weights\")\n",
        "model.save(\"yolov3.h5\")"
      ],
      "metadata": {
        "id": "quh50ZUMQ9BB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ddc17b-c2cb-4c41-fd5f-ebc26cf6ab59"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Crop + Save crop data\n",
        "- 학습 속도가 더욱 빨라지도록 사람을 인식하고 해당되는 영역만 자름"
      ],
      "metadata": {
        "id": "b0DD-OOfNYgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dr) # 현위치 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krALWrsTUHcc",
        "outputId": "be32419b-94c5-4b93-a59b-b2eaaa696f92"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1zaWNDIbEB7mz9mfiWjgvZ9_Vanr9-sSZ/DATA/STAND1500/TRAIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다.\n",
        "\n",
        "for filename in glob.glob(dr):\n",
        "  print(filename)\n",
        "  name_list = filename.split(\"/\")\n",
        "  print(name_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isb8JHumVA5M",
        "outputId": "35530c35-d98f-446a-e813-382e0ba29cc0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1zaWNDIbEB7mz9mfiWjgvZ9_Vanr9-sSZ/DATA/STAND1500/TRAIN\n",
            "['', 'content', 'gdrive', '.shortcut-targets-by-id', '1zaWNDIbEB7mz9mfiWjgvZ9_Vanr9-sSZ', 'DATA', 'STAND1500', 'TRAIN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(name_list[len(name_list) - 1])\n",
        "print(drD)\n",
        "print(name_list)\n",
        "print(name_list[len(name_list) - 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQiq45y9YhsB",
        "outputId": "96fdbe6f-0b5e-4eec-beb8-05f5952f9a49"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN\n",
            "/content/gdrive/.shortcut-targets-by-id/1zaWNDIbEB7mz9mfiWjgvZ9_Vanr9-sSZ/DATA/STAND1500/TRAIN/*/*.jpg\n",
            "['', 'content', 'gdrive', '.shortcut-targets-by-id', '1zaWNDIbEB7mz9mfiWjgvZ9_Vanr9-sSZ', 'DATA', 'STAND1500', 'TRAIN']\n",
            "STAND1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import core\n",
        "import colorsys\n",
        "import random\n",
        "\n",
        "\n",
        "def draw_bbox(file_name1, bboxes, show_label=True):\n",
        "    Msc=[0]\n",
        "  \n",
        "    try:\n",
        "        classes = class_names #classes : 딕셔너리 가능 항목 가져옴 num:str\n",
        "    except NameError:\n",
        "        classes = core.utils.read_class_names(\"./data/classes/coco.names\")\n",
        "    num_classes = len(classes)\n",
        "    image_h, image_w, _ = file_name1.shape#이미지 크기\n",
        "\n",
        "    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "\n",
        "    random.seed(0)\n",
        "    random.shuffle(colors)\n",
        "    random.seed(None)\n",
        "\n",
        "    for i, bbox in enumerate(bboxes):#indexing(count, list v)\n",
        "        coor = np.array(bbox[:4], dtype=np.int32)\n",
        "        \n",
        "        fontScale = 0.5\n",
        "        score = bbox[4]\n",
        "        Msc=np.append(Msc,score)\n",
        "        class_ind = int(bbox[5])\n",
        "\n",
        "        if classes[class_ind] in \"person\" :\n",
        "          if score>=np.max(Msc):\n",
        "            bbox_color = colors[class_ind]\n",
        "            bbox_thick = int(0.5 * (image_h + image_w) / 600)\n",
        "            c1, c2 = (coor[0], coor[1]), (coor[2], coor[3]) \n",
        "            print(c1,\"/\",c2)\n",
        "            cv2.rectangle(file_name1, c1, c2, bbox_color, bbox_thick)\n",
        "            score=np.max(Msc)\n",
        "            print('%s'%classes[class_ind],\"SC\",score)\n",
        "            bbox_mess = '%s: %.2f' % (classes[class_ind], score)#출력되는 text\n",
        "            t_size = cv2.getTextSize(bbox_mess, 0, fontScale, thickness=bbox_thick//2)[0]\n",
        "\n",
        "            c1_1= (c1[0] + t_size[0], c1[1] - t_size[1] - 3)\n",
        "            PP=file_name1[c1[1]:c2[1],c1[0]:c2[0]]\n",
        "\n",
        "            cv2.rectangle(file_name1,c1, (c1[0] + t_size[0], c1[1] - t_size[1] - 3-100), bbox_color,0)  # filled#labeling2\n",
        "            #이미지S 파일,시작점(x,y),종료점(x,y)\n",
        "            PP = cv2.cvtColor(PP, cv2.COLOR_RGB2BGR) # 색상 변경 # /content/gdrive/MyDrive/PROJECT/DATA0826/STAND/TRAIN/\n",
        "            cv2.imwrite(\"/content/gdrive/MyDrive/TTEST/TRAIN/\" +name_list[len(name_list) - 2]+\"/\"+ name_list[len(name_list) - 1], PP) # 사람 부분만 이미지 추출 #자기의 tmp 폴더에 good 폴더 미리 생성 \n",
        "    return file_name1"
      ],
      "metadata": {
        "id": "qrstlraGREVh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in glob.glob(drD):  # \"현 디렉터리\"의 모든 사진들을 불러옴\n",
        "  print(file_name)\n",
        "  name_list = file_name.split(\"/\")\n",
        "  new_filename = name_list[len(name_list) - 1] # 사진 파일 이름을 new_filename에 저장\n",
        "  INPUT_SIZE = 800\n",
        "\n",
        "  image_path = file_name\n",
        "  file_name = cv2.imread(image_path)#이미지 파일 경로\n",
        "  file_name = cv2.cvtColor(file_name, cv2.COLOR_BGR2RGB)#입력 이미지,색상 변환 코드\n",
        "\n",
        "  plt.figure(figsize=(12,20))\n",
        "  plt.imshow(file_name)\n",
        "  plt.show()\n",
        "\n",
        "  #2\n",
        "  bboxes = model.predict(file_name)#keras코드에서 boundary box로 bounding 됨\n",
        "\n",
        "  #3\n",
        "  dummy = np.copy(file_name)\n",
        "  print(bboxes)\n",
        "  for box in bboxes:\n",
        "    x1, y1, x2, y2 = int(box[0]), int(box[1]), int(box[2]), int(box[3])\n",
        "    conf = box[4]\n",
        "    cls_id = int(box[5])\n",
        "    dummy = cv2.rectangle(dummy, (x1,y1), (x2,y2), (255,0,0), 2)#빨간색, 선의 두께는 2\n",
        "\n",
        "  plt.figure(figsize=(12,20))#(그래프 가로, 세로)\n",
        "  file_name = draw_bbox(file_name, bboxes)\n",
        "print(\"\\nALL DONE\")\n",
        "  "
      ],
      "metadata": {
        "id": "vQmvuKA7ZS1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DEEP LEARNING(CNN)\n",
        "\n",
        "- vanilla cnn을 사용, 가장 기본적으로 사용되는 방식\n",
        "- 위에서 잘린 stnad 이미지 : https://drive.google.com/drive/folders/1emfX_5RaR7MvrP_pdgOhdYFrbp3ikPhF?usp=sharing"
      ],
      "metadata": {
        "id": "J169Mn5lwKQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Module importing"
      ],
      "metadata": {
        "id": "Aogk90XlJuye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, BatchNormalization, Conv2D, MaxPooling2D, Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.regularizers import l1, l2, L1L2\n",
        "import time\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications import efficientnet\n",
        "from tensorflow.keras.applications import EfficientNetB2\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MYUpPu5jwJpY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Input((224,224,3)))\n",
        "model.add(Conv2D(64, (3,3), padding='same'))\n",
        "model.add(Conv2D(64, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(256, (3,3), padding='same'))\n",
        "model.add(Conv2D(256, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(512, (3,3), padding='same'))\n",
        "model.add(Conv2D(512, (3,3), padding='same'))\n",
        "model.add(Conv2D(512, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(512, (3,3), padding='same'))\n",
        "model.add(Conv2D(512, (3,3), padding='same'))\n",
        "model.add(Conv2D(512, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu',kernel_regularizer=l2()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(4, activation='softmax'))"
      ],
      "metadata": {
        "id": "1g4MR_hMwocy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#실시간loss그래프\n",
        "from IPython.display import clear_output\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class PlotLosses(Callback):\n",
        "\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.epochs = []\n",
        "    self.losses = []\n",
        "    self.val_losses = []\n",
        "    self.logs = []\n",
        "    self.fig = plt.figure()\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "    self.epochs.append(epoch)\n",
        "    self.losses.append(logs.get('loss'))\n",
        "    self.val_losses.append(logs.get('val_loss'))\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    plt.plot(self.epochs, self.losses, label=\"loss\")\n",
        "    plt.plot(self.epochs, self.val_losses, label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.show();\n",
        "    print(\"loss = \", self.losses[-1], \", val_loss = \", self.val_losses[-1])"
      ],
      "metadata": {
        "id": "JYYYKKMX7Qz4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses = PlotLosses()"
      ],
      "metadata": {
        "id": "8wLo5Ge5KErI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Learning"
      ],
      "metadata": {
        "id": "fsYxgd4JKGxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#다중분류\n",
        "\n",
        "from tensorflow .keras. optimizers import Adam\n",
        "optimizer = Adam(learning_rate= 0.001*0.1, decay = 0.01 ) #lr\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "def normalize(image):\n",
        "  return image/255.0\n",
        "\n",
        "preprocessor = normalize\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_data_generator = ImageDataGenerator(\n",
        "      # rotation_range=10,\n",
        "      rotation_range=4, #bad1~3,good 4개\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=False,\n",
        "      preprocessing_function=preprocessor\n",
        ").flow_from_directory(\n",
        "      \"/content/gdrive/MyDrive/DATA0829/STAND1500/TRAIN\",#학습시킬 경로\n",
        "      target_size=(224,224),\n",
        "      batch_size=BATCH_SIZE,\n",
        "      class_mode='sparse'\n",
        ")\n",
        "\n",
        "test_data_generator = ImageDataGenerator(\n",
        "      preprocessing_function=preprocessor\n",
        ").flow_from_directory(\n",
        "      \"/content/gdrive/MyDrive/DATA0829/STAND1500/TEST\", #test경로\n",
        "      target_size=(224,224),\n",
        "      batch_size=BATCH_SIZE,\n",
        "      class_mode='sparse'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "6ngbr7ChKHRW",
        "outputId": "0fa5a2fb-186b-4eb4-ca24-e09274d457c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_75 (Conv2D)          (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_76 (Conv2D)          (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " batch_normalization_72 (Bat  (None, 224, 224, 64)     256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_77 (Conv2D)          (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_78 (Conv2D)          (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " batch_normalization_73 (Bat  (None, 112, 112, 128)    512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_79 (Conv2D)          (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_80 (Conv2D)          (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_74 (Bat  (None, 56, 56, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_81 (Conv2D)          (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv2d_82 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_83 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " batch_normalization_75 (Bat  (None, 28, 28, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_84 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_85 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_86 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " batch_normalization_76 (Bat  (None, 14, 14, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 7, 7, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                1605696   \n",
            "                                                                 \n",
            " batch_normalization_77 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " batch_normalization_78 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,741,124\n",
            "Trainable params: 15,737,924\n",
            "Non-trainable params: 3,200\n",
            "_________________________________________________________________\n",
            "Found 2687 images belonging to 4 classes.\n",
            "Found 513 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "model_check_point = ModelCheckpoint(\n",
        "    'pose_best_model.h5', \n",
        "    monitor='val_loss', \n",
        "    verbose=1, \n",
        "    save_best_only=True)\n",
        "#모델체크포인트\n",
        "\n",
        "'''\n",
        "tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath, monitor='val_loss', verbose=0, save_best_only=False,\n",
        "    save_weights_only=False, mode='auto', save_freq='epoch', options=None, **kwargs\n",
        ")\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "Mn9b1RXVKVn6",
        "outputId": "3e41eb89-66e5-4294-f231-cae7eb2b955c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntf.keras.callbacks.ModelCheckpoint(\\n    filepath, monitor='val_loss', verbose=0, save_best_only=False,\\n    save_weights_only=False, mode='auto', save_freq='epoch', options=None, **kwargs\\n)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_data_generator,\n",
        "      validation_data=test_data_generator,\n",
        "      steps_per_epoch=train_data_generator.samples/BATCH_SIZE,\n",
        "      validation_steps=test_data_generator.samples/BATCH_SIZE,      \n",
        "      epochs=250, callbacks=[model_check_point,plot_losses]\n",
        ")\n",
        "\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "test_x, test_y = next(iter(test_data_generator))\n",
        "y_ = model.predict(test_x)\n",
        "predicted = np.argmax(y_, axis=-1)\n",
        "\n",
        "plt.plot(test_y[:100], \"o\")\n",
        "plt.plot(predicted[:100], '.')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZNKwAXqZKY46",
        "outputId": "13bd3b0d-36c4-475b-a21b-e59acfb60e4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n"
          ]
        }
      ]
    }
  ]
}